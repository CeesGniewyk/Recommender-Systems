{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-3.2.3.Discussion.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CeesGniewyk/Recommender-Systems/blob/master/Assignment_3_2_3_Discussion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIsAlou8A-KR",
        "colab_type": "text"
      },
      "source": [
        "## Assignment 3.2. Image Caption Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YVsdA0OhVJtX"
      },
      "source": [
        "Cees Gniewyk - 0859034 <br>\n",
        "Celine Senden - 0865165 <br>\n",
        "Timothy de Vries - 0740585"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2s1hBW8A-KV",
        "colab_type": "text"
      },
      "source": [
        "## Task 2.3: Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf6mm1imA-KY",
        "colab_type": "text"
      },
      "source": [
        "1.Briefly describe one of the preceding works on modeling Image-Caption according to the paper and its limitation. Name the advantage(s) of the current Image-Caption generator as compared to the previous work?\n",
        "\n",
        "\n",
        "2.Given an image with multiple text descriptions, how to represent this pair of data as model input-output?\n",
        "\n",
        "-Explain the answer in a notation (x, y) or clear description that guarantees you can run the same model with your input-output representation. Name all the input(s) and output(s) of the model.\n",
        "\n",
        "-How does the model extract image features from raw images? Name and briefly explain how to employ the feature extractor. Include the dimension (array shape) of the extracted features.\n",
        "\n",
        "-How are the train descriptions represented into the model? Why do we need to add “starting” and “ending” token of every caption in a preprocessing stage?\n",
        "\n",
        "-How does the model initialize RNN states for caption generator? What is being fed into the decoder states in the first time step (t = 0)?\n",
        "\n",
        "3.What is the motivation of incorporating Beam Search in Sequence-to-Sequence learning? Briefly explain how the method works in an inference stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6IcMNwUA-Kb",
        "colab_type": "text"
      },
      "source": [
        "### YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLUTb8CzA-Ke",
        "colab_type": "text"
      },
      "source": [
        "1.An example of a previous work in Image Caption Generation is the work of Li et al. [Composing simple image descriptions using web-scale n-grams]. In this work, they use image recognition techniques to extract objects, attributes and spatial relationships from the given image. This visual content is then encoded as a set of triples; each triple describing a pair of detected objects and their relationship. From these triples they then generate natural language desriptions in two steps: (n-gram) phrase selection and (n-gram) phrase fusion. Phrase selection collects candidate phrases that may be potentially useful for generating the description of a given image. Phrase fusion finds the optimal compatible set of phrases to compose a new (and more complex) phrase that describes the image.\n",
        "So in short, they start off with image detections, and then piece together a final description using phrases containing the detected objects and their relationships. \n",
        "This means that they make use of so called \"template-based\" text generation, as most previous works do. This technique is limited, in the sense that it lacks the attempt to generate novel disriptions and therefor cannot accurately describe previously unseen compositions of objects, even if the individual objects are encountered during training.\n",
        "The advantage of the current Image-Caption generator as compared to this previous work is that this is one single joint model that is able to create captions for unseen images without using pre-defined (sub)sentences, but by created a sentence from scratch using words from a dictionary.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.\n",
        "\n",
        "-Given an image with multiple text descriptions, this pair of data can be described as model input-output in the following way: the image is inputted multiple times with a different caption. So with 5 text descriptions, the image - caption combination has 5 duplicate images with 5 unique captions.\n",
        "\n",
        "-.(image 1, text 1), (image 1, text 2), (image 1,text 3),(image 1,text 4), (image 1,text 5), (image 2, text 1) .......\n",
        "\n",
        "-The train desciptions are represented into the model as embedded word vectors. Adding “starting” and “ending” tokens to every caption in the preprocessing stage is needed, because -- the model needs to learn which words are logical starting and ending words, besides only learning which words logically follow each other and also to know when to stop generating words at inference stage, since the sentence (caption) is done as soon as the model generates an \"ending\" token (?) --\n",
        "\n",
        " At timestep t=0, the last hidden layer of the CNN is the input to the RNN decoder. With teacher forcing, the previous captions are fed into the model, so at time t=1 the actual caption at time 0 is also fed into the model.\n",
        "\n",
        "\n",
        "3.The motivation behind incorporating Beam Seach in Sequence-to-Sequence learning is to reduce the number of sub-optimal output sequences created by the model. This phenomenom is a consequence of Greedy Search; an algorithm that just picks the token with the highest probability at every spot, but these best probabilities at every spot do not nessecarily add up to the best overall (joint) probability of the whole output sequence. Beam Search tries to tackle this by introducing a parameter B (beam width) and at every step keeping a top B of most likely possibilities, instead of just one. So in the first step, Beam Search will create a top B most likely candidates for the first word, in the second step it will create a top B of most likely sequences of the first and the second word together, etc."
      ]
    }
  ]
}