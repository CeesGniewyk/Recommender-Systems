{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-3.1.3.Discussion.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CeesGniewyk/Recommender-Systems/blob/master/Assignment_3_1_3_Discussion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTniZpSqA0gw",
        "colab_type": "text"
      },
      "source": [
        "## Assignment 3.1. Sequence Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YVsdA0OhVJtX"
      },
      "source": [
        "Cees Gniewyk - 0859034 <br>\n",
        "Celine Senden - 0865165 <br>\n",
        "Timothy de Vries - 0740585"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv_LNoEcA0g0",
        "colab_type": "text"
      },
      "source": [
        "## Task 1.3: Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USQKyu9UA0g3",
        "colab_type": "text"
      },
      "source": [
        "1.What is the motivation of incorporating an \"attention mechanism\" in a Machine Translation task? What is the main issue that this attention trying to solve? Mention the advantage(s) as compared to the model without attention.\n",
        "\n",
        "\n",
        "2.Likewise, what is the motivation of adding an \"attention\" network in aspect-level sentiment classification? What is the main issue that this attention trying to solve? Mention the advantage(s) as compared to the model without attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zffLRPSCA0g5",
        "colab_type": "text"
      },
      "source": [
        "### YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbjpqC7yA0g7",
        "colab_type": "text"
      },
      "source": [
        "1.The motivation of incorporating an \"attention mechanism\" in Machine Translation is the difficulty for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus. The main issue that this attention is trying to solve is catastrophic forgetting; the problem that the decoder receives only the last encoder hidden state (the final output of the encoder), which than is hoped to be a sufficient vector representation summary of the whole input sequence. In practice, all the necessary information of an input sequence compressed into a fixed-length vector actually cannot possibly capture and 'remember' all parts of the input, especially for long input sequences. The advantage of implementing attention in Machine Translation, as compared to the model without attention, is that the model can now cope with long input sentences and translate also those effectively. This is achieved because attention provides the decoder with information from almost Ã©very encoder hidden state, enabling the model to selectively focus on useful parts of the input sequence and learn the alignment between them.\n",
        "\n",
        "\n",
        "2.The motivation of adding an \"attention\" network in aspect-level sentiment classification is the challenge to separate different opinion contexts for different aspects (targets), since a sentence can contain multiple aspects with different corresponding sentiments (multiple sentiment-target pairs). The main issue that this attention is trying to solve is finding the right context word(s) to focus on when inferring the sentiment polarity of the given target aspect. The advantage(s) as compared to the model without attention is a better capturing of the relavant opinion contect for a given aspect when there are multiple aspects in the sentence, which leads to more accurate aspect-level sentiment classifications."
      ]
    }
  ]
}